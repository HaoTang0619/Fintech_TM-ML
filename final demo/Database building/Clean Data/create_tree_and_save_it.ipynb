{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "#這個cell是我為你們寫好的function\n",
    "\n",
    "def Sent_Data_To_Mysql(tagged_article, source_name, tag_type,keyword):\n",
    "    db = pymysql.connect(host=\"18.217.252.187\",port=3306, user=\"public_author\",passwd=\"antimoneylaunderingisgood2\",db=\"AML_News\" ,charset='utf8')\n",
    "    content = db.escape_string(tagged_article)\n",
    "    try:\n",
    "        with db.cursor() as cursor:\n",
    "            sql0 = \"SELECT `id` FROM `\"+source_name+\"` WHERE '\"+tagged_article+\"' LIKE CONCAT('%', `tagged_article`, '%')\"\n",
    "            cursor.execute(sql0)\n",
    "            duplicate_id = cursor.fetchone()\n",
    "            if(duplicate_id is not None):#如果已經存在一個content重複的\n",
    "                sql1 = \" INSERT INTO `\"+source_name+\"` (`id`, `tagged_article`,`tag_type`,`keyword`) VALUES (%s,%s,%s,%s) \"\n",
    "                sql2 = \" ON DUPLICATE KEY UPDATE `tagged_article`= VALUES(`tagged_article`),`tag_type`= VALUES(`tag_type`),`keyword`= VALUES(`keyword`)\"\n",
    "                cursor.execute(sql1+sql2, (str(duplicate_id[0]), tagged_article,tag_type,keyword))\n",
    "                print(str(duplicate_id[0]))\n",
    "            else:#如果沒有重複的\n",
    "                sql = \"INSERT INTO `\"+source_name+\"`(`tagged_article`,`tag_type`,`keyword`) VALUES ( %s,%s,%s)\"\n",
    "                cursor.execute(sql, (tagged_article,tag_type,keyword))\n",
    "        db.commit()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def Get_Data_From_Mysql(source_name,keyword):\n",
    "    contents_list=[]\n",
    "    target_ids =[]\n",
    "    db = pymysql.connect(host=\"18.217.252.187\",port=3306, user=\"public_author\",passwd=\"antimoneylaunderingisgood2\",db=\"AML_News\" ,charset='utf8')\n",
    "    try:\n",
    "        with db.cursor() as cursor:\n",
    "            if(keyword==\"all\"):\n",
    "                sql0 = \"SELECT * FROM `\"+source_name+\"`\"\n",
    "                cursor.execute(sql0)\n",
    "                contents_list = cursor.fetchall()\n",
    "            else: \n",
    "                sql0 = \"SELECT * FROM `\"+source_name+\"` WHERE '\"+keyword+\"' LIKE CONCAT('%', `keyword`)\"\n",
    "                cursor.execute(sql0)\n",
    "                contents_list = cursor.fetchall()\n",
    "                \n",
    "        db.commit()\n",
    "    finally:\n",
    "        db.close()\n",
    "    return  pd.DataFrame(list(contents_list), columns=['id','tagged_article','tag_type','keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_sentence_and_nameIdx(tagged_article):\n",
    "    sentence_and_nameIdx = []\n",
    "    tagged_article = tagged_article.replace('\\n', '').replace(' ', '').replace('。', '。\\a').replace('《', '').replace('》', '').split('\\a')\n",
    "    for tagged_sentence in tagged_article:\n",
    "        if tagged_sentence.find('/nr') < 0:\n",
    "            continue\n",
    "        sentence = tagged_sentence.replace('/nr', '').replace('/nt', '').replace('/ns', '').replace('/o', '')\n",
    "        sentences = tagged_sentence\n",
    "        name_list = []\n",
    "        for findIter in re.finditer('/nr', tagged_sentence):\n",
    "            start_idx = tagged_sentence[:findIter.start()].rfind('/o') + 2\n",
    "            end_idx = findIter.start()\n",
    "            name = tagged_sentence[start_idx:end_idx]\n",
    "            if name not in name_list:\n",
    "                name_list.append(name)\n",
    "        names_and_idxs = []\n",
    "        for name in name_list:\n",
    "            name_and_idx = [name]\n",
    "            for findIter in re.finditer(name, sentence):\n",
    "                name_and_idx.append([findIter.start(), findIter.end()])\n",
    "            names_and_idxs.append(name_and_idx)\n",
    "        sentence_and_nameIdx.append([sentences, sentence, names_and_idxs])\n",
    "    return sentence_and_nameIdx\n",
    "\n",
    "def keep_name_right(sentence):\n",
    "    \"\"\"\n",
    "    用tag過NER的句子斷句\n",
    "    再把每一個斷詞送進結巴切詞\n",
    "    希望盡可能保持姓名完整\n",
    "    EX:\n",
    "    input: 阮/nr 姓女子為首的/o 偽鈔集團/nt ，該集團今年二月已被/o 雲林/ns 警方偵破，\n",
    "            起出新台幣千元假鈔二百六十萬元，但已有三十五萬元流入市面。警方陸續逮捕十二名涉案嫌犯，包括兩名在學校/o 福利社/nt\n",
    "    \n",
    "    用NER斷句 >> 阮, 姓女子為首的, 偽鈔集團, ，該集團今年二月已被,...\n",
    "    \n",
    "    ouput(用結巴斷詞) >> 阮, 姓, 女子, 為首, 的, 偽鈔, 集團, ... \n",
    "    \"\"\"\n",
    "    Sentence = []\n",
    "    word_list = []\n",
    "    temps = sentence.replace('nt',' ').replace('nr',' ').replace('o',' ')\n",
    "    temps = temps.split(' ')\n",
    "    for te in temps:\n",
    "        Sentence.append(str(te).replace('/',''))\n",
    "    #tempss = temps.replace('/','')\n",
    "    Sentence\n",
    "    for t in Sentence:\n",
    "        to_convert= str(t)\n",
    "        cc = OpenCC('t2s')  # (Optional )convert from Simplified Chinese to Traditional Chinese\n",
    "        content = cc.convert(to_convert)#一樣轉成簡體字，這樣Stanford的系統才會比較準確\n",
    "        #res = segmenter.segment(content)\n",
    "        word_list.append(list(jieba.cut(content, cut_all=False)))\n",
    "        #word_list.append(list(segmenter.segment(content)))\n",
    "        #temps.split('/')\n",
    "    word_list = sum(word_list,[])\n",
    "    return(word_list)\n",
    "\n",
    "def tag_POS(tree):\n",
    "    \"\"\"\n",
    "    忽略這個\n",
    "    \"\"\"\n",
    "    POS_tag = []\n",
    "    #tree = tree.pos()\n",
    "    for item in tree:\n",
    "        word,POS = str(item).replace('(','').replace(')','').replace(\"'\",'').split(',')\n",
    "        POS_tag.append([word,POS])\n",
    "    return POS_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_sentence_and_nameIdx(tagged_article):\n",
    "    sentence_and_nameIdx = []\n",
    "    tagged_article = tagged_article.replace('\\n', '').replace(' ', '').replace('。', '。\\a').replace('《', '').replace('》', '').split('\\a')\n",
    "    for tagged_sentence in tagged_article:\n",
    "        if tagged_sentence.find('/nr') < 0:\n",
    "            continue\n",
    "        sentence = tagged_sentence.replace('/nr', '').replace('/nt', '').replace('/ns', '').replace('/o', '')\n",
    "        sentences = tagged_sentence\n",
    "        name_list = []\n",
    "        for findIter in re.finditer('/nr', tagged_sentence):\n",
    "            start_idx = tagged_sentence[:findIter.start()].rfind('/o') + 2\n",
    "            end_idx = findIter.start()\n",
    "            name = tagged_sentence[start_idx:end_idx]\n",
    "            if name not in name_list:\n",
    "                name_list.append(name)\n",
    "        names_and_idxs = []\n",
    "        for name in name_list:\n",
    "            name_and_idx = [name]\n",
    "            for findIter in re.finditer(name, sentence):\n",
    "                name_and_idx.append([findIter.start(), findIter.end()])\n",
    "            names_and_idxs.append(name_and_idx)\n",
    "        sentence_and_nameIdx.append([sentences, sentence, names_and_idxs])\n",
    "    return sentence_and_nameIdx\n",
    "\n",
    "def keep_name_right(sentence):\n",
    "    \"\"\"\n",
    "    用tag過NER的句子斷句\n",
    "    再把每一個斷詞送進結巴切詞\n",
    "    希望盡可能保持姓名完整\n",
    "    EX:\n",
    "    input: 阮/nr 姓女子為首的/o 偽鈔集團/nt ，該集團今年二月已被/o 雲林/ns 警方偵破，\n",
    "            起出新台幣千元假鈔二百六十萬元，但已有三十五萬元流入市面。警方陸續逮捕十二名涉案嫌犯，包括兩名在學校/o 福利社/nt\n",
    "    \n",
    "    用NER斷句 >> 阮, 姓女子為首的, 偽鈔集團, ，該集團今年二月已被,...\n",
    "    \n",
    "    ouput(用結巴斷詞) >> 阮, 姓, 女子, 為首, 的, 偽鈔, 集團, ... \n",
    "    \"\"\"\n",
    "    Sentence = []\n",
    "    word_list = []\n",
    "    temps = sentence.replace('nt',' ').replace('nr',' ').replace('o',' ')\n",
    "    temps = temps.split(' ')\n",
    "    for te in temps:\n",
    "        Sentence.append(str(te).replace('/',''))\n",
    "    #tempss = temps.replace('/','')\n",
    "    Sentence\n",
    "    for t in Sentence:\n",
    "        to_convert= str(t)\n",
    "        cc = OpenCC('t2s')  # (Optional )convert from Simplified Chinese to Traditional Chinese\n",
    "        content = cc.convert(to_convert)#一樣轉成簡體字，這樣Stanford的系統才會比較準確\n",
    "        #res = segmenter.segment(content)\n",
    "        word_list.append(list(jieba.cut(content, cut_all=False)))\n",
    "        #word_list.append(list(segmenter.segment(content)))\n",
    "        #temps.split('/')\n",
    "    word_list = sum(word_list,[])\n",
    "    return(word_list)\n",
    "\n",
    "def tag_POS(tree):\n",
    "    \"\"\"\n",
    "    忽略這個\n",
    "    \"\"\"\n",
    "    POS_tag = []\n",
    "    #tree = tree.pos()\n",
    "    for item in tree:\n",
    "        word,POS = str(item).replace('(','').replace(')','').replace(\"'\",'').split(',')\n",
    "        POS_tag.append([word,POS])\n",
    "    return POS_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smile\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "跑文法樹前一定要跑這個!!!\n",
    "\"\"\"\n",
    "#結巴斷詞、NLTK文法樹\n",
    "import os\n",
    "import jieba\n",
    "from opencc import OpenCC\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import StanfordSegmenter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "#這是比較直覺(攏長)的用法，直接指定路徑\n",
    "chi_tagger = StanfordPOSTagger('./StanfordNLP/models/chinese-distsim.tagger',\n",
    "                               './StanfordNLP/jars/stanford-ner.jar')\n",
    "segmenter = StanfordSegmenter(\n",
    "    java_class='edu.stanford.nlp.ie.crf.CRFClassifier',\n",
    "    path_to_jar=\"./StanfordNLP/jars/stanford-segmenter-3.9.2.jar\",\n",
    "    path_to_slf4j=\"./StanfordNLP/jars/slf4j-api.jar\",\n",
    "    path_to_sihan_corpora_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data\",\n",
    "    path_to_model=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/pku.gz\",\n",
    "    path_to_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/dict-chris6.ser.gz\"\n",
    ")\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-12.0.1\"#注意這邊你們電腦要安裝java jdk，並放入你們自己的jdk的安裝路徑\n",
    "os.environ[\"CLASSPATH\"] = \".\\StanfordNLP\\stanford-parser-2018-10-17\"\n",
    "os.environ[\"STANFORD_MODELS\"] = \"./StanfordNLP/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立文法樹並儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_tree_and_save_it(i, crime_name):\n",
    "    #跑文法樹\n",
    "        #sentences: 保有NER tag\n",
    "        #sentence: 原型句子\n",
    "        #name_and_idx: 姓名、位置\n",
    "    for sentences, sentence, name_and_idx in i:\n",
    "        if crime_name in sentence: #如果犯罪名稱有在句子裡\n",
    "            if len(sentence) < 500:      #如果犯罪名稱長度小於500\n",
    "                word_list = keep_name_right(sentences) \n",
    "                #print(word_list)\n",
    "                ch_parser = StanfordParser(model_path='./StanfordNLP/models/chinesePCFG.ser.gz')\n",
    "                sent = ch_parser.parse(word_list) #形成文法樹\n",
    "                pickle.dump(sent, file) #用pickle存\n",
    "                #for sen in sent:\n",
    "                    #tree_list = sen.pos() \n",
    "    return print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 舉例: 要存蘋果新聞裡與洗錢有關的 (我只跑了database[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\smile\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.797 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "source_name = ['apple_daily','china_times', 'et_today', 'liberty_times', 'united_daily', 'judicial_gov']\n",
    "output_dataframe = Get_Data_From_Mysql(source_name[0],'all') #洗錢為例\n",
    "crime_name = ['人口販運', '性剝削', '偽造貨幣', '殺人', '重傷害', '搶奪' , '勒索' , '海盜', '恐怖主義', '資恐', '非法販運武器'\n",
    "              , '贓物', '竊盜', '綁架', '拘禁', '妨礙自由', '環保犯罪', '偽造文書', '仿冒', '侵害商業秘密', '毒品犯運', '詐欺'\n",
    "              , '走私', '稅務犯罪', '組織犯罪', '證卷犯罪', '貪汙賄賂', '洗錢']\n",
    "#篩選句子內有犯罪名稱的句子\n",
    "crime = (output_dataframe['keyword'] == crime_name[27]) #####這裡要改，(舉洗錢為例)\n",
    "database = []\n",
    "for art in output_dataframe[crime][\"tagged_article\"]:\n",
    "    database.append(find_sentence_and_nameIdx(art)) #找出姓名，連成list\n",
    "#畫文法樹並存在pickle\n",
    "file = open('tree.pickle', 'wb')\n",
    "for i in database[0:10]: #####可以像我先跑個幾筆([0:10])試一下，再跑全部，全部就是直接跑 >> for i in database\n",
    "    create_tree_and_save_it(i, crime_name[27])   #####這裡要改，(舉洗錢為例)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取tree, 轉成tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讀取完畢\n"
     ]
    }
   ],
   "source": [
    "tree_POS = []\n",
    "with open('tree.pickle', 'rb') as file:\n",
    "    while 1:\n",
    "        try:\n",
    "            tree = pickle.load(file)\n",
    "            for i in tree:\n",
    "                tree_POS.append(list(i.pos()))\n",
    "        except:\n",
    "            print(\"讀取完畢\")\n",
    "            break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('研讨会', 'NN'), ('由', 'P'), ('法务部', 'NN'), ('检察', 'NN'), ('司', 'NN'), ('副司长', 'NN'), ('兼', 'CC'), ('洗钱', 'NN'), ('防制', 'NN'), ('办公室', 'NN'), ('执行', 'NN'), ('秘书', 'NN'), ('余丽贞', 'NN'), ('与', 'CC'), ('调查局', 'NN'), ('长', 'NN'), ('蔡清祥', 'NN'), ('开幕', 'VV'), ('，', 'PU'), ('也', 'AD'), ('勉励', 'VV'), ('检察机关', 'NN'), ('等', 'ETC'), ('执法人员', 'NN'), ('，', 'PU'), ('善用', 'VV'), ('金融', 'NN'), ('情报', 'NN'), ('与', 'CC'), ('资金', 'NN'), ('调查', 'NN'), ('手段', 'NN'), ('，', 'PU'), ('持续', 'VV'), ('精进', 'NN'), ('犯罪', 'NN'), ('金', 'JJ'), ('流', 'NN'), ('侦查', 'NN'), ('技巧', 'NN'), ('，', 'PU'), ('落实', 'VV'), ('政府', 'NN'), ('（', 'PU'), ('全民', 'NN'), ('防制', 'NR'), ('洗钱', 'NN'), ('、', 'PU'), ('追查', 'NN'), ('犯罪', 'NN'), ('金', 'JJ'), ('流', 'NN'), ('）', 'PU'), ('的', 'DEG'), ('政策', 'NN'), ('，', 'PU'), ('以', 'P'), ('彻底', 'JJ'), ('打击', 'NN'), ('洗钱', 'NN'), ('、', 'PU'), ('资恐', 'NR'), ('及', 'CC'), ('相关', 'JJ'), ('犯罪活动', 'NN'), ('，', 'PU'), ('以及', 'CC'), ('加强', 'VV'), ('跨部门', 'JJ'), ('合作', 'NN'), ('，', 'PU'), ('因应', 'NN'), ('未来', 'NT'), ('新兴', 'VV'), ('与', 'P'), ('跨境', 'NN'), ('犯罪活动', 'NN'), ('的', 'DEG'), ('发展趋势', 'NN'), ('。', 'PU')]\n",
      "------\n",
      "[('行政院', 'NN'), ('政务委员', 'NN'), ('罗秉成', 'NN'), ('表示', 'VV'), ('，', 'PU'), ('过去', 'NT'), ('台湾', 'NR'), ('ns', 'NN'), ('一度', 'AD'), ('因为', 'P'), ('反洗钱', 'NN'), ('作为', 'VV'), ('落后', 'VV'), ('于', 'P'), ('其他', 'DT'), ('国家', 'NN'), ('，', 'PU'), ('掉入', 'NR'), ('（', 'PU'), ('加强', 'VV'), ('追踪', 'VV'), ('）', 'PU'), ('名单', 'NN'), ('，', 'PU'), ('不过', 'AD'), ('在', 'P'), ('这', 'DT'), ('12', 'CD'), ('天', 'M'), ('，', 'PU'), ('近', 'AD'), ('100', 'CD'), ('场', 'M'), ('、', 'PU'), ('参与', 'VV'), ('人次', 'M'), ('2000', 'CD'), ('多人次', 'NN'), ('的', 'DEG'), ('面谈', 'NN'), ('中', 'LC'), ('，', 'PU'), ('无论是', 'CS'), ('场次', 'NN'), ('或是', 'AD'), ('参与', 'VV'), ('人次', 'NN'), ('，', 'PU'), ('都', 'AD'), ('下', 'DT'), ('创新', 'NN'), ('高', 'VA'), ('，', 'PU'), ('所有', 'DT'), ('公', 'JJ'), ('、', 'PU'), ('私', 'JJ'), ('部门', 'NN'), ('全力', 'JJ'), ('努力', 'NN'), ('的', 'DEG'), ('结果', 'NN'), ('令', 'VV'), ('印象', 'NN'), ('深刻', 'VA'), ('，', 'PU'), ('评鉴', 'NN'), ('团', 'NN'), ('高度肯定', 'NN'), ('国家', 'NN'), ('高度', 'JJ'), ('支持', 'NN'), ('下', 'LC'), ('，', 'PU'), ('国家', 'NN'), ('的', 'DEG'), ('跨部门', 'JJ'), ('协调', 'NN'), ('机制', 'NN'), ('运作', 'NN'), ('良好', 'VA'), ('。', 'PU')]\n",
      "------\n",
      "[('前', 'JJ'), ('央行', 'NN'), ('总裁', 'NN'), ('彭', 'NR'), ('淮南', 'NR'), ('先前', 'AD'), ('曾', 'AD'), ('公开', 'AD'), ('表示', 'VV'), ('比特', 'NR'), ('币', 'NN'), ('交易', 'NN'), ('应', 'VV'), ('列入', 'VV'), ('洗钱', 'NN'), ('防', 'VV'), ('制法', 'NN'), ('通报', 'NN'), ('系统', 'NN'), ('，', 'PU'), ('去年底', 'NN'), ('还', 'AD'), ('写信给', 'VV'), ('法务部', 'NN'), ('长', 'JJ'), ('邱太', 'NN'), ('三盼', 'CD'), ('重视', 'NN'), ('虚拟', 'JJ'), ('货币', 'NN'), ('的', 'DEG'), ('洗钱', 'NN'), ('问题', 'NN'), ('。', 'PU')]\n",
      "------\n",
      "[('检方', 'NN'), ('已依涉', 'NN'), ('犯', 'VV'), ('诈欺', 'NN'), ('、', 'PU'), ('洗钱罪', 'VV'), ('嫌', 'VV'), ('拘', 'NN'), ('提到', 'VV'), ('台湾', 'NR'), ('ns', 'NN'), ('民', 'NN'), ('政府', 'NN'), ('那', 'DT'), ('发起人', 'NN'), ('林志升', 'NN'), ('夫妇', 'NN'), ('，', 'PU'), ('目前', 'NT'), ('搜索', 'NN'), ('持续', 'VV'), ('中', 'LC'), ('，', 'PU'), ('据悉', 'AD'), ('已', 'AD'), ('查扣', 'VV'), ('民', 'NN'), ('政府', 'NN'), ('内部资料', 'NN'), ('、', 'PU'), ('及', 'CC'), ('黑熊', 'VV'), ('部队', 'NN'), ('使用', 'NN'), ('的', 'DEG'), ('枪枝', 'NN'), ('等', 'ETC'), ('证物', 'NN'), ('，', 'PU'), ('之后', 'AD'), ('将', 'AD'), ('陆续', 'AD'), ('带回', 'VV'), ('桃园', 'NR'), ('地检署', 'NN'), ('进行', 'VV'), ('侦讯', 'NN'), ('。', 'PU')]\n",
      "------\n",
      "[('（', 'PU'), ('突发', 'JJ'), ('中心', 'NN'), ('／', 'PU'), ('桃园', 'NR'), ('报导', 'NN'), ('）', 'PU'), ('出版', 'NN'), ('时间', 'NN'), ('11', 'NT'), ('：', 'PU'), ('24', 'CD'), ('修改', 'NN'), ('时间', 'NN'), ('17', 'CD'), ('：', 'PU'), ('560000', 'CD'), ('想', 'VV'), ('知道', 'VV'), ('更', 'AD'), ('多', 'VA'), ('，', 'PU'), ('一定', 'AD'), ('要', 'VV'), ('看', 'VV'), ('…', 'PU'), ('…', 'PU'), ('2018051013512830000', 'CD'), ('【', 'PU'), ('调查', 'NN'), ('片', 'NN'), ('】', 'PU'), ('检调', 'NN'), ('搜索', 'VV'), ('台湾', 'NR'), ('ns', 'NR'), ('民', 'NN'), ('政府', 'NN'), ('带回', 'VV'), ('数十', 'CD'), ('箱', 'M'), ('证物', 'NN'), ('2018051013514910000', 'CD'), ('遭控', 'NN'), ('洗钱', 'NN'), ('及', 'CC'), ('诈骗', 'NN'), ('民', 'NN'), ('政府', 'NN'), ('林志升', 'NR'), ('家中', 'NN'), ('藏上', 'NN'), ('亿元', 'CD'), ('0000', 'CD'), ('看', 'VV'), ('了', 'AS'), ('这', 'DT'), ('则', 'M'), ('新闻', 'NN'), ('的', 'DEC'), ('人', 'NN'), ('，', 'PU'), ('也', 'AD'), ('看', 'VV'), ('了', 'AS'), ('…', 'PU'), ('…', 'PU'), ('2018051013509910000', 'CD'), ('中华', 'NR'), ('ns', 'NR'), ('电信', 'NN'), ('推', 'VV'), ('499', 'CD'), ('吃', 'VV'), ('到', 'VV'), ('饱', 'AD'), ('方案', 'NN'), ('连', 'AD'), ('立院', 'NN'), ('也', 'AD'), ('现疯', 'VV'), ('抢', 'VV'), ('人潮', 'NN'), ('2018051013509680000', 'CD'), ('【', 'PU'), ('有片', 'JJ'), ('】', 'PU'), ('柯力', 'NN'), ('挺', 'AD'), ('连任', 'VV'), ('总统', 'NN'), ('释', 'NN'), ('善意', 'NN'), ('蔡', 'NR'), ('英文', 'NN'), ('：', 'PU'), ('莫忘', 'NN'), ('初衷', 'NN'), ('2018051013509710000', 'CD'), ('\\u200b', 'NN'), ('侯友宜', 'NN'), ('跑', 'VV'), ('宜兰', 'NR'), ('ns', 'NN'), ('反', 'JJ'), ('深澳', 'NN'), ('电厂', 'NN'), ('陈金德', 'NR'), ('用', 'P'), ('一句', 'CD'), ('俗语', 'NN'), ('反讽', 'NR'), ('桃园', 'NR'), ('地检署', 'NN'), ('桃园', 'NR'), ('地检署', 'NN'), ('今', 'DT'), ('指挥', 'NN'), ('龟', 'JJ'), ('山', 'NN'), ('分局', 'NN'), ('前往', 'VV'), ('台湾', 'NR'), ('ns', 'NN'), ('民', 'NN'), ('政府', 'NN'), ('搜索', 'NN'), ('。', 'PU')]\n",
      "------\n",
      "[('金管会', 'NR'), ('主委', 'NN'), ('顾立雄', 'NR'), ('今天', 'NT'), ('早上', 'NT'), ('也', 'AD'), ('坦言', 'VV'), ('，', 'PU'), ('税务', 'NN'), ('当然', 'AD'), ('是', 'VC'), ('涉及', 'VV'), ('洗钱', 'NN'), ('疑虑', 'NN'), ('。', 'PU')]\n",
      "------\n",
      "[('李震华', 'NN'), ('／', 'PU'), ('司法', 'NN'), ('改革', 'NN'), ('关怀', 'NN'), ('互助会', 'NN'), ('理事长', 'NN'), ('、', 'PU'), ('律师', 'NN'), ('我国', 'NN'), ('自从', 'P'), ('1997', 'NT'), ('年', 'NT'), ('加入', 'VV'), ('亚太', 'NR'), ('防制', 'NN'), ('洗钱', 'NN'), ('组织', 'NN'), ('以来', 'LC'), ('，', 'PU'), ('屡屡', 'AD'), ('被', 'SB'), ('评鉴', 'VV'), ('为', 'P'), ('加强', 'VV'), ('追踪', 'VV'), ('国家', 'NN'), ('或', 'CC'), ('不', 'AD'), ('合作', 'VV'), ('国家', 'NN'), ('之坏', 'NN'), ('评', 'VV'), ('，', 'PU'), ('这', 'DT'), ('是因为', 'CD'), ('台湾', 'NR'), ('ns', 'VV'), ('防制', 'NN'), ('洗钱', 'NN'), ('的', 'DEG'), ('法律', 'NN'), ('制定', 'VV'), ('落后', 'VA'), ('，', 'PU'), ('更', 'AD'), ('因', 'P'), ('台湾', 'NR'), ('ns', 'NN'), ('现实', 'NN'), ('环境', 'NN'), ('与', 'P'), ('绝大多数', 'CD'), ('的', 'DEG'), ('国家', 'NN'), ('没有', 'VE'), ('邦交', 'NN'), ('，', 'PU'), ('导致', 'VV'), ('金流', 'JJ'), ('的', 'DEG'), ('查缉', 'NN'), ('非常', 'AD'), ('困难', 'VA'), ('，', 'PU'), ('也', 'AD'), ('导致', 'VV'), ('台湾', 'NR'), ('ns', 'NN'), ('的', 'DEG'), ('经济', 'NN'), ('犯', 'VV'), ('屡屡', 'AD'), ('用人', 'VV'), ('头', 'DT'), ('犯罪', 'NN'), ('的', 'DEG'), ('手法', 'NN'), ('来', 'MSP'), ('洗钱', 'VV'), ('，', 'PU'), ('状况', 'NN'), ('时有所闻', 'VV'), ('。', 'PU')]\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for item in tree_POS:\n",
    "    print(item)\n",
    "    print('------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
